## 19. Kafka消息的幂等性和事务是怎么实现的？
### 生产者幂等
producer在生产发送消息时，有可能会retry导致重复发送消息，**Producer 的幂等性指的是当发送同一条消息时，数据在 Server 端只会被持久化一次，数据不丟不重**。
为了实现Producer的幂等性，kafka引入了Producer ID（即PID)和Sequence Number。

PID：每个producer在初始化的时候都会被分配一个唯一的PID，这个PID对应用透明，完全没有暴露给用户，**对于一个给定的PID，sequence number将会从0开始自增，
每个topic-partition都会有一个独立的sequence number**，producer在发送数据时，将会给msg标识一个sequence number，Server也就是通过这个验证数据是否重复，
**这里的PID是全局唯一，producer故障后重新启动后会被分配一个新的PID，这也是幂等性无法做到跨会话的一个原因**。

Sequence Number：**有PID之后，在PID+topic-partition级别上添加一个sequence numbers信息，就可以实现producer的幂等性**。

#### 幂等性前后对比
理想情况下消息发送流程：  
![idempotent1](https://img-blog.csdnimg.cn/20200926224058515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzIxMzgzNDM1,size_16,color_FFFFFF,t_70#pic_center)  

出现网络异常时  
![idempotent2](https://pic3.zhimg.com/80/v2-98a0483aaed887d85aabb2bbba5f8816_720w.jpg)  

引入幂等性后  
![idempotent3](https://pic1.zhimg.com/80/v2-972919d67d896eb730ff04e81f6e69e0_720w.jpg)  

### 事务
引入序列号来实现幂等也只是针对每一对而言的，也就是说，**Kafka的幂等只能保证单个生产者会话（session）中单分区的幂等。幂等性不能跨多个分区运作，而事务可以弥补这个缺陷**。  	
**事务可以保证对多个分区写入操作的原子性。操作的原子性是指多个操作要么全部成功，要么全部失败，不存在部分成功、部分失败的可能。**
	
为了使用事务，应用程序必须提供唯一的transactionalId，这个transactionalId通过客户端参数transactional.id来显式设置。事务要求生产者开启幂等特性，
因此通过**将transactional.id参数设置为非空从而开启事务特性的同时，需要将enable.idempotence设置为true**（如果未显式设置，则KafkaProducer默认会将它的值设置为true），
如果用户显式地将enable.idempotence设置为false，则会报出ConfigException的异常。

transactionalId与PID一一对应，两者之间所不同的是**transactionalId由用户显式设置，而PID是由Kafka内部分配的**。
另外，为了保证新的生产者启动后具有相同transactionalId的旧生产者能够立即失效，每个生产者通过transactionalId获取PID的同时，还会获取一个单调递增的producer epoch。
如果使用同一个transactionalId开启两个生产者，那么前一个开启的生产者会报错。

**从生产者的角度分析，通过事务，Kafka可以保证跨生产者会话的消息幂等发送，以及跨生产者会话的事务恢复**。
前者表示具有相同transactionalId的新生产者实例被创建且工作的时候，旧的且拥有相同transactionalId的生产者实例将不再工作。  
后者指当某个生产者实例宕机后，新的生产者实例可以保证任何未完成的旧事务要么被提交（Commit），要么被中止（Abort），如此可以使新的生产者实例从一个正常的状态开始工作。
 
	
#### Kafka事务的使用
Kafka中的事务特性主要用于以下两种场景：  
* 生产者发送多条消息可以封装在一个事务中，形成一个原子操作。多条消息要么都发送成功，要么都发送失败。
* read-process-write模式：将消息消费和生产封装在一个事务中，形成一个原子操作。在一个流式处理的应用中，常常一个服务需要从上游接收消息，然后经过处理后送达到下游，这就对应着消息的消费和生成。  
当事务中仅仅存在Consumer消费消息的操作时，它和Consumer手动提交Offset并没有区别。因此**单纯的消费消息并不是Kafka引入事务机制的原因，单纯的消费消息也没有必要存在于一个事务中**。

Kafka producer API提供了以下接口用于事务操作：
````javascript
void initTransactions();
void beginTransaction() throws ProducerFencedException;
void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offsets, String consumerGroupId) throws ProducerFencedException;
void commitTransaction() throws ProducerFencedException;
void abortTransaction() throws ProducerFencedException;

initTransactions()方法用来初始化事务；
beginTransaction()方法用来开启事务；
sendOffsetsToTransaction()方法为消费者提供在事务内的位移提交的操作；
commitTransaction()方法用来提交事务；
abortTransaction()方法用来中止事务，类似于事务回滚。
````
在消费端有一个参数isolation.level，与事务有着莫大的关联，这个参数的默认值为“read_uncommitted”，意思是说消费端应用可以看到（消费到）未提交的事务，当然对于已提交的事务也是可见的。
这个参数还可以设置为“read_committed”，表示消费端应用不可以看到尚未提交的事务内的消息。
举个例子，如果生产者开启事务并向某个分区值发送3条消息msg1、msg2和msg3，**在执行commitTransaction()或abortTransaction()方法前，设置为“read_committed”的消费端应用是消费不到这些消息的**，
不过在KafkaConsumer内部会缓存这些消息，直到生产者执行commitTransaction()方法之后它才能将这些消息推送给消费端应用。反之，如果生产者执行了abortTransaction()方法，
那么KafkaConsumer会将这些缓存的消息丢弃而不推送给消费端应用。

#### Kafka事务特性
Kafka的事务特性本质上代表了三个功能：原子写操作，拒绝僵尸实例（Zombie fencing）和读事务消息。
##### 原子写
Kafka的事务特性本质上是支持了Kafka跨分区和Topic的原子写操作。在同一个事务中的消息要么同时写入成功，要么同时写入失败。
我们知道，Kafka中的Offset信息存储在一个名为_consumed_offsets的Topic中，因此**read-process-write模式，除了向目标Topic写入消息，还会向_consumed_offsets中写入已经消费的Offsets数据。
因此read-process-write本质上就是跨分区和Topic的原子写操作。Kafka的事务特性就是要确保跨分区的多个写操作的原子性**。

##### 拒绝僵尸实例（Zombie fencing）
在分布式系统中，一个instance的宕机或失联，集群往往会自动启动一个新的实例来代替它的工作。此时若原实例恢复了，那么集群中就产生了两个具有相同职责的实例，
此时前一个instance就被称为“僵尸实例（Zombie Instance）”。在Kafka中，两个相同的producer同时处理消息并生产出重复的消息（read-process-write模式），这样就严重违反了Exactly Once Processing的语义。这就是僵尸实例问题。

Kafka事务特性通过transaction-id属性来解决僵尸实例问题。**所有具有相同transaction-id的Producer都会被分配相同的pid，同时每一个Producer还会被分配一个递增的epoch**。
Kafka收到事务提交请求时，如果检查当前事务提交者的epoch不是最新的，那么就会拒绝该Producer的请求。从而达成拒绝僵尸实例的目标。

##### 读事务消息
为了保证事务特性，Consumer如果设置了isolation.level = read_committed，那么它只会读取已经提交了的消息。在Producer成功提交事务后，Kafka会将所有该事务中的消息的Transaction Marker从uncommitted标记为committed状态，从而所有的Consumer都能够消费。

##### 总结
* Transaction Marker与PID提供了识别消息是否应该被读取的能力，从而实现了事务的隔离性。
* Offset的更新标记了消息是否被读取，从而将对读操作的事务处理转换成了对写（Offset）操作的事务处理。
* Kafka事务的本质是，将一组写操作（如果有）对应的消息与一组读操作（如果有）对应的Offset的更新进行同样的标记（Transaction Marker）来实现事务中涉及的所有读写操作同时对外可见或同时对外不可见。
* kafka只提供对Kafka本身的读写操作的事务性，不提供包含外部系统的事务性。
* **生产环境基本不会开启事务，一般都是使用confirm机制**

https://www.jianshu.com/p/64c93065473e  
https://zhuanlan.zhihu.com/p/69039887  
https://www.cnblogs.com/fnlingnzb-learner/p/13646390.html  
https://blog.csdn.net/qq_21383435/article/details/108818553  
<https://www.cnblogs.com/husterindg/p/10696885.html>  
