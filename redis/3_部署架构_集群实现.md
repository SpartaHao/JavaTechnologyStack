## 20. Redis的主从复制？Redis有几种部署模式？重点讲下集群和哨兵机制的实现？
### Redis集群方案的三种模式：
1. 主从复制模式能实现**读写分离，但是不能自动故障转移**；
2. 哨兵模式**基于主从复制模式，能实现自动故障转移，达到高可用**，但与主从复制模式一样，**不能在线扩容，容量受限于单机的配置**；
3. Cluster模式通过无中心化架构，实现**分布式存储，可进行线性扩展，也能高可用，但对于像批量操作、事务操作等的支持性不够好**。

无论是主从模式，还是哨兵模式，这两个模式都有一个问题，**不能水平扩容，并且这两个模式的高可用特性都会受到Master主节点内存的限制**。三种模式各有优缺点，可根据实际场景进行选择。

### 主从复制模式
基本原理：主从复制模式中包含一个主数据库实例（master）与一个或多个从数据库实例（slave），如下图  

![主从](https://img-blog.csdnimg.cn/20200413163358928.png)  

**客户端可对主数据库进行读写操作，对从数据库进行读操作，主数据库写入的数据会实时自动同步给从数据库**。

具体工作机制为：
* slave启动后，向master发送SYNC命令，master接收到SYNC命令后通过bgsave保存快照（即上文所介绍的RDB持久化），并**使用缓冲区记录保存快照这段时间内执行的写命令**
* master将保存的快照文件发送给slave，并继续记录执行的写命令
* slave接收到快照文件后，加载快照文件，载入数据
* master快照发送完后开始向slave发送缓冲区的写命令，slave接收命令并执行，完成复制初始化
* 此后master每次执行一个写命令都会同步发送给slave，保持master与slave之间数据的一致性

注：配置主从命令：slaveof 127.0.0.1:6379。真实的主从配置应该是在配置文件里面配置（replicaof 127.0.0.1 6379），这样的话才是永久的。  
执行info replication命令可以查看连接该数据库的其它库的信息

#### 主从复制的优缺点
优点：
* master能自动将数据同步到slave，可以进行**读写分离**，分担master的读压力
* master、slave之间的同步是以**非阻塞的方式进行的，同步期间，客户端仍然可以提交查询或更新请求**

缺点：
* **不具备自动容错与恢复功能**，master或slave的宕机都可能导致客户端请求失败，需要等待机器重启或手动切换客户端IP才能恢复
* master宕机，如果宕机前数据没有同步完，则切换IP后会存在**数据不一致**的问题
* **难以支持在线扩容，Redis的容量受限于单机配置**
* 存储海量的数据，那么BGSAVE指令生成的RDB文件会非常巨大，这个文件传送给从节点也会非常慢，如果缓冲区命令很多的话，从节点同步数据时也会执行很久


### Sentinel（哨兵）模式
基本原理：哨兵模式**基于主从复制模式，只是引入了哨兵来监控与自动处理故障**。如图

![sentinel](https://img-blog.csdnimg.cn/20200413163359607.png)

哨兵顾名思义，就是来为Redis集群站哨的，一旦发现问题能做出相应的应对处理。其功能包括:
* 监控master、slave是否正常运行
* 当master出现故障时，能自动将一个slave转换为master（大哥挂了，选一个小弟上位）
* 多个哨兵可以监控同一个Redis，哨兵之间也会自动监控

哨兵模式的具体工作机制:
在配置文件中通过sentinel monitor <master-name> <ip> <redis-port> <quorum>来定位master的IP、端口，一个哨兵可以监控多个master数据库，只需要提供多个该配置项即可。哨兵启动后，会与要监控的master建立两条连接：
*  一条连接用来订阅master的sentinel:hello频道与获取其他监控该master的哨兵节点信息
*  另一条连接定期向master发送**INFO等命令获取master本身的信息**

与master建立连接后，哨兵会执行三个操作：
*  定期（一般10s一次，当master被标记为主观下线时，改为1s一次）向master和slave发送INFO命令
*  定期向master和slave的sentinel:hello频道发送自己的信息
*  定期（1s一次）向master、slave和其他哨兵发送PING命令

> * 如果被PING的数据库或者节点超时（超过 down-after-milliseconds 选项所指定的值）未回复，哨兵认为其**主观下线**（sdown，s就是Subjectively —— 主观地）。
> * 如果下线的是master，哨兵会向其它哨兵发送命令询问它们是否也认为该master主观下线，**如果达到一定数目（即配置文件中的quorum）投票，
  哨兵会认为该master已经客观下线（odown，o就是Objectively —— 客观地），**并选举领头的哨兵节点对主从系统发起故障恢复**。
> * 若没有足够的sentinel进程同意master下线，master的客观下线状态会被移除，若master重新向sentinel进程发送的PING命令返回有效回复，master的主观下线状态就会被移除。

#### 客观下线后的故障处理：
哨兵认为master客观下线后，故障恢复的操作需要由选举的领头哨兵来执行，选举采用**Raft算法**：
* **发现master下线的哨兵节点**（我们称他为A）向每个哨兵发送命令，要求对方选自己为领头哨兵
* 如果目标哨兵节点没有选过其他人，则会同意选举A为领头哨兵
* 如果有**超过一半**的哨兵同意选举A为领头，则A当选
* 如果有多个哨兵节点同时参选领头，此时有可能存在一轮投票无竞选者胜出，此时每个参选的节点**等待一个随机时间**后再次发起参选请求，进行下一轮投票竞选，直至选举出领头哨兵

选出领头哨兵后，领头者开始对系统进行故障恢复，从出现故障的master的从节点中挑选一个来当选新的master,选择规则如下：
* 断开连接时长,如果连接断开的比较久，超过了某个阈值，就直接失去了选举权
* 如果有选举权，则从所有在线的slave中选择优先级最高的，优先级可以通过slave-priority配置
* 如果有多个最高优先级的slave，则选取复制偏移量最大（即复制越完整）的当选
* 如果以上条件都一样，选取id最小的slave
* 挑选出需要继任的slave后，领头哨兵向该数据库发送命令使其升格为master，然后再向其他slave发送命令接受新的master，最后更新数据。将已经停止的旧的master更新为新的master的从数据库，使其恢复服务后以slave的身份继续运行。

#### 哨兵模式的优缺点：
优点：
* 哨兵模式基于主从复制模式，所以主从复制模式有的优点，哨兵模式也有。**哨兵模式就是主从模式的升级**，手动到自动，更加健壮
* 哨兵模式下，master挂掉可以自动进行切换，系统可用性更高

缺点：
* 同样也继承了主从模式难以在线扩容的缺点，Redis的容量受限于单机配置（过于依赖master）
* 需要**额外的资源来启动sentinel进程，实现相对复杂一点，同时slave节点作为备份节点不提供服务**

### Cluster模式
基本原理：哨兵模式解决了主从复制不能自动故障转移，达不到高可用的问题，但还是存在难以在线扩容，Redis容量受限于单机配置的问题。如果有存储海量数据的需求，同步会非常缓慢，
  所以我们应该**把一个主从结构变成多个，把存储的key分摊到各个主从结构中来分担压力，演进为集群版本**。Cluster模式实现了Redis的**分布式存储，即每台节点存储不同的内容，来解决在线扩容的问题**。如图
  
![cluster](https://img-blog.csdnimg.cn/20200413163400164.png)

Cluster采用无中心结构,它的特点如下：
* 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽
* 节点的fail是通过集群中超过半数的节点检测失效时才生效
* 客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可

Cluster模式的具体工作机制：
* 在Redis的每个节点上，都有一个插槽（slot），取值范围为0-16383
* 当我们存取key的时候，Redis会根据**CRC16**的算法得出一个结果，然后把结果对16384求余数，这样每个key都会对应一个编号在0-16383之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作
* 为了保证高可用，Cluster模式也引入主从复制模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点
* 当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点都宕机了，那么该集群就无法再提供服务了
* Cluster模式集群节点最小配置6个节点(3主3从，因为需要半数以上)，其中**主节点提供读写操作，从节点作为备用节点，不提供请求，只作为故障转移使用**。

#### Cluster模式的优缺点
优点：
* 无中心架构，数据按照slot分布在多个节点。
* 集群中的每个节点都是平等的关系，每个节点都保存各自的数据和整个集群的状态。每个节点都和其他所有节点连接，而且这些连接保持活跃，这样就保证了我们只需要连接集群中的任意一个节点，就可以获取到其他节点的数据。
* 可线性扩展到1000多个节点，**节点可动态添加或删除**（redis 的动态扩容操作都是通过redis源码文件夹中, redis-trib.rb脚本文件来完成的, 运行此脚本需要 ruby环境）
* 能够实现自动故障转移，节点之间通过gossip协议交换状态信息，用投票机制完成slave到master的角色转换

缺点：
* 客户端实现复杂，驱动要求实现Smart Client，缓存slots mapping信息并及时更新，提高了开发难度。目前仅JedisCluster相对成熟，异常处理还不完善，比如常见的“max redirect exception”
* 节点会因为某些原因发生阻塞（阻塞时间大于 cluster-node-timeout）被判断下线，这种failover是没有必要的
* **数据通过异步复制，不保证数据的强一致性**
* slave充当“冷备”，不能缓解读压力
* **批量操作限制**，目前**只支持具有相同slot值的key执行批量操作**，对mset、mget、sunion等操作支持不友好
* key事务操作支持有限，**只支持多key在同一节点的事务操作**，多key分布不同节点时无法使用事务功能
* 不支持多数据库空间，单机redis可以支持16个db，集群模式下只能使用一个，即db 0 Redis Cluster模式不建议使用pipeline和multi-keys操作，减少max redirect产生的场景。

参考文件：

https://blog.csdn.net/qianlia/article/details/105491761（图片地址）

https://www.cnblogs.com/pingyeaa/p/11294773.html

## 21. redis有哪些集群方案？Codis和Redis集群的区别？
Redis集群方案目前主流的有三种，分别是Twemproxy、Codis和Redis Cluster。
* Twemproxy – Twitter
* Codis – 豌豆荚
* Redis Cluster – 官方

Twemproxy,是推特开源的，它最大的缺点就是无法平滑的扩缩容，而**Codis解决了Twemproxy扩缩容的问题**，而且兼容了Twemproxy，它是由豌豆荚开源的，和Twemproxy都是代理模式。其实Codis能发展起来的一个主要原因是它是在Redis官方集群方案漏洞百出的时候率先成熟稳定的。以现在的Redis官方集群方案，这两个好像没有太大差别了，不过我也没有去做性能测试，不清楚哪个最好。
Redis Cluster是由官方出品的，用去中心化的方式实现，不属于代理模式。

### Twemproxy
大概概念是，它**类似于一个代理方式**，使用方法和普通redis无任何区别，设置好它下属的多个redis实例后，使用时在本需要连接redis的地方改为连接twemproxy，
它会以一个代理的身份接收请求并使用一致性hash算法，将请求转接到具体redis，将结果再返回twemproxy。使用方式简便(相对redis只需修改连接端口)，对旧项目扩展的首选。
问题：twemproxy自身单端口实例的压力，使用一致性hash后，对redis节点数量改变时候的计算值的改变，数据无法自动移动到新的节点。

### codis
目前用的最多的集群方案，基本和twemproxy一致的效果，但它支持在 节点数量改变情况下，旧节点数据可恢复到新hash节点。
Codis 是一个分布式 Redis 解决方案, 对于上层的应用来说, 连接到** Codis Proxy** 和连接原生的 Redis Server 没有明显的区别 (不支持的命令列表), 上层应用可以像使用单机的 Redis 一样使用,
  Codis 底层会处理请求的转发, 不停机的数据迁移等工作, 所有后边的一切事情, 对于前面的客户端来说是透明的, 可以简单的认为后边连接的是一个内存无限大的 Redis 服务.

在Codis里面，它把所有的key分为1024个槽，每一个槽位都对应了一个分组，具体槽位的分配，可以进行自定义，现在如果有一个key进来，首先要根据CRC32算法，针对key算出32位的哈希值，然后除以1024取余，然后就能算出这个KEY属于哪个槽，然后根据槽与分组的映射关系，就能去对应的分组当中处理数据了。

#### **槽位的映射关系是保存在proxy里面的**，不同proxy之间怎么同步映射关系？
**在Codis中使用Zookeeper来保存映射关系**（其实就是地址，名称，pid，启动时间、state等信息），由proxy上来同步配置信息。

#### codis proxy如果出现异常怎么处理？
这个可能要利用一下k8s中pod的特性，在k8s里面可以设置pod冗余的数量，k8s会严格保证启动的数量与设置一致，所以只需要一个进程监测Proxy的异常，并且把它干掉就可以了，k8s会自动拉起来一个新的proxy。
  codis给这个进程起名叫codis-ha，**codis-ha实时监测proxy的运行状态，如果有异常就会干掉**，它包含了哨兵的功能，所以豌豆荚直接把哨兵去掉了。

### redis cluster
它和codis不太一样，**Codis它是通过代理分片的，但是Redis Cluster是去中心化的没有代理，所以只能通过客户端分片，它分片的槽数跟Codis不太一样，Codis是1024个，而Redis cluster有16384个，槽跟节点的映射关系保存在每个节点上，而codis是保存在proxy上**，每个节点每秒钟会ping十次其他几个最久没通信的节点，其他节点也是一样的原理互相PING ，PING的时候一个是判断其他节点有没有问题，另一个是顺便交换一下当前集群的节点信息、包括槽与节点映射的关系等。

客户端操作key的时候先通过分片算法算出所属的槽，然后随机找一个服务端请求。**客户端可以向任一实例发出请求，如果所需数据不在该实例中，就会返回一个MOVED ERROR通知，引导客户端去正确的节点访问，这个时候客户端就会去正确的节点操作数据**。

codis和redis cluster对比来看的话：
类型 | codis | redis cluster
---- | ----- | -----
数据库数量： | 16 | 1
不支持的命令：| keys | select
Dashboart：| 有 | 没有
可视化客户端：| 有 | 没有
集群结构：| 代理 | 去中心化
哈希槽：| 1024 | 16384
升级：| 无法保证后续升级 | 官方正品有保证
部署：| 较复杂 | 简单

参考文件：

https://www.cnblogs.com/pingyeaa/p/11294773.html
  
https://www.iteye.com/blog/boyseegirl-2388191
